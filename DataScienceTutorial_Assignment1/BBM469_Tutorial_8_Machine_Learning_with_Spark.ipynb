{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/aucan/DataScienceTutorials/blob/master/BBM469_Tutorial_8_Machine_Learning_with_Spark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hn9xpRGlF8Rv"
   },
   "source": [
    "# BBM469-Data Intensive Applications Lab. Machine Learning with Spark\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15yLVsGsIbme"
   },
   "source": [
    "**The Aim**\n",
    "\n",
    "In this document, you will be given for some tutorials and headlines for you to learn the basics of Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TnWTEJ0dIiIW"
   },
   "source": [
    "**Apache Spark**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "02Xb_GkfG3jF"
   },
   "source": [
    "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n",
    "\n",
    "Further more information: https://spark.apache.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuqpvhx9Jh5e"
   },
   "source": [
    "**Running Pyspark in Colab**\n",
    "\n",
    "To run spark in Colab, we need to first install all the dependencies in Colab environment i.e. Apache Spark 2.4.5 with hadoop 2.7, Java 8 and Findspark to locate the spark in the system. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NSNFy0gSF8HD"
   },
   "source": [
    "First, install the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fSBRCj74tmma"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open 'spark-2.4.5-bin-hadoop2.7.tgz'\n",
      "'pip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NWdHdtlYGJTu"
   },
   "source": [
    " Set the location of Java and Spark by running the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j4V0y9g2txv6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X_s4eqkXGWO4"
   },
   "source": [
    "Test your installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f8QupaNxtz_e"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3e51e97f758b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfindspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local[*]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python38\\lib\\site-packages\\findspark.py\u001b[0m in \u001b[0;36minit\u001b[1;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# add pyspark to sys.path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[0mspark_python\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_home\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m     \u001b[0mpy4j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'lib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'py4j-*.zip'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DcFZRR8UG9bh"
   },
   "source": [
    "**Clustering Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UUw5yO7tJSQH"
   },
   "source": [
    "**K-means**\n",
    "\n",
    "K-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. Further more information: https://spark.apache.org/docs/latest/ml-clustering.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M6NAKl0wH1af"
   },
   "source": [
    "import the following libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ui1EyTGXt71v"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python38\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.datasets.samples_generator module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.datasets. Anything that cannot be imported from sklearn.datasets is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6a1f20d60c41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmpl_toolkits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmplot3d\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAxes3D\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples_generator\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_blobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVJi2RNIKejy"
   },
   "source": [
    "Generate some input data and write the dataset as a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HvZ-C49uNfi"
   },
   "outputs": [],
   "source": [
    "n_samples=10000\n",
    "n_features=3\n",
    "X, y = make_blobs(n_samples=n_samples, centers=10, n_features=n_features, random_state=42)\n",
    "\n",
    "# add a row index as a string\n",
    "pddf = pd.DataFrame(X, columns=['x', 'y', 'z'])\n",
    "pddf['id'] = 'row'+pddf.index.astype(str)\n",
    "\n",
    "#move it first (left)\n",
    "cols = list(pddf)\n",
    "cols.insert(0, cols.pop(cols.index('id')))\n",
    "pddf = pddf.ix[:, cols]\n",
    "pddf.head()\n",
    "\n",
    "# save the ndarray as a csv file\n",
    "pddf.to_csv('input.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ITK9mNuH0vOw"
   },
   "source": [
    "List your files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yV9s8qqHnNjk"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N02CbC16KuC6"
   },
   "source": [
    "Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2QszSThDudMF"
   },
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(X[:,0], X[:,1], X[:,2], c=y)\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wy0LNsocLOlF"
   },
   "source": [
    "This is a visualization of the data we just generated, where the true type of each data point is represented by a unique color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AY8S-25KLp5-"
   },
   "source": [
    "**SQL context**\n",
    "\n",
    "Spark SQL is a Spark module for structured data processing. Unlike the basic Spark RDD API, the interfaces provided by Spark SQL provide Spark with more information about the structure of both the data and the computation being performed. Further more information: https://spark.apache.org/docs/latest/sql-programming-guide.html\n",
    "\n",
    "What this means is that we can use Spark dataframes, which are similar to Pandas dataframes, and is a dataset organized into named columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lxSncwQv_RAG"
   },
   "source": [
    "Create sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IsDbES0bujgB"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OHllA6SMgDR"
   },
   "source": [
    "**Read in data from CSV into a Spark data frame**\n",
    "\n",
    "The data contains four columns, 'id', 'x', 'y', 'z', and it is the latter three that we want to use as features in our clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c8IJGKjuuyuu"
   },
   "outputs": [],
   "source": [
    "FEATURES_COL = ['x', 'y', 'z']\n",
    "path = 'input.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnzzZWzZu21h"
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(path, header=True)\n",
    "print (df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iK__s1QSOFv0"
   },
   "source": [
    "**Convert all data columns to float**\n",
    "\n",
    "The dataframe consists now of four columns of strings. Converting all data to float is possible in a single line. However this would make the 'id' column filled with null, or we would have to omit it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kDy8gAvtv8-n"
   },
   "outputs": [],
   "source": [
    "df_feat = df.select(*(df[c].cast(\"float\").alias(c) for c in df.columns[1:]))\n",
    "df_feat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L_IPDU5KOl44"
   },
   "source": [
    "Since we know which columns need to be converted, we get a cleaner result by converting those one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxHSS7dBwBJ-"
   },
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if col in FEATURES_COL:\n",
    "        df = df.withColumn(col,df[col].cast('float'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U92SE2P4O2rS"
   },
   "source": [
    "Drop the null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OaJ-x3RowGgB"
   },
   "outputs": [],
   "source": [
    "df = df.na.drop()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K5YWhSifO_gC"
   },
   "source": [
    "**Create a features column to be used in the clustering**\n",
    "\n",
    "Spark's implementation of KMeans is a bit different from for example scikit-learn's version. We need to store all features as an array of floats, and store this array as a column called \"features\". Since we do no longer need the original columns we filter them out with a select statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iz8ZsJ7RwJXY"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=FEATURES_COL, outputCol=\"features\")\n",
    "df_kmeans = vecAssembler.transform(df).select('id', 'features')\n",
    "df_kmeans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uE0z3MbkPaKG"
   },
   "source": [
    "**Optimize choice of k**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4k-lmdPPaD9"
   },
   "source": [
    "To optimize k we cluster a fraction of the data for different choices of k and look for an \"elbow\" in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LtpBMpoJwNNr"
   },
   "outputs": [],
   "source": [
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(df_kmeans.sample(False,0.1, seed=42))\n",
    "    cost[k] = model.computeCost(df_kmeans) # requires Spark 2.0 or later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Infg6-cmwPst"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize =(8,6))\n",
    "ax.plot(range(2,20),cost[2:20])\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('cost')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_PQlc4-zP0pb"
   },
   "source": [
    "Look like there is very little gain after k=12, so we stick to that choice when processing the full data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eEbaXmTzQASe"
   },
   "source": [
    "**Train the machine learning model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QzrZWThwS48"
   },
   "outputs": [],
   "source": [
    "k = 12\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(df_kmeans)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2CMc4AJQOEV"
   },
   "source": [
    "**Assign clusters to event**\n",
    "\n",
    "We need to assign the individual rows to the nearest cluster centroid. That can be done with the transform method, which adds 'prediction' column to the dataframe. The prediction value is an integer between 0 and k, but it has no correlation to the y value of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kOIKsXNwVpB"
   },
   "outputs": [],
   "source": [
    "transformed = model.transform(df_kmeans).select('id', 'prediction')\n",
    "rows = transformed.collect()\n",
    "print(rows[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aXjf5WPxRWHp"
   },
   "source": [
    "From the rows returned by the collect method create a new dataframe using our SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iq5ZM4AawX81"
   },
   "outputs": [],
   "source": [
    "df_pred = sqlContext.createDataFrame(rows)\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nUKPFOpMR0Km"
   },
   "source": [
    "Join the prediction with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THqrVpc-waeX"
   },
   "outputs": [],
   "source": [
    "df_pred = df_pred.join(df, 'id')\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AoWKP7zsR3j7"
   },
   "source": [
    "Convert to Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHZz78Fawc4Z"
   },
   "outputs": [],
   "source": [
    "pddf_pred = df_pred.toPandas().set_index('id')\n",
    "pddf_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9RNYLW2cU4of"
   },
   "source": [
    "Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3qHfTX6wfu6"
   },
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(pddf_pred.x, pddf_pred.y, pddf_pred.z, c=pddf_pred.prediction)\n",
    "threedee.set_xlabel('x')\n",
    "threedee.set_ylabel('y')\n",
    "threedee.set_zlabel('z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7XYa__EYPi7"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PjbwtVDUYL7B"
   },
   "source": [
    "**Classification Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9J8KuTV2j0Q7"
   },
   "source": [
    "Download the dataset into the colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eYODZD7Gb_3j"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve('http://web.utk.edu/~wfeng1/spark/_static/WineData2.csv', 'WineData2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ySP63h0jjX3"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DUcVMB5Poa8Y"
   },
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m5pN_-tYkK_4"
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"WineData2.csv\")\n",
    "df1.show(5)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VFsTYPa3rLc2"
   },
   "outputs": [],
   "source": [
    "COLS = ['fixed','volatile','citric','sugar','chlorides','free','total','density','pH','sulphates','alcohol','quality']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5vISz02EyCMV"
   },
   "source": [
    "Convert all data columns to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhHim8kdrnUN"
   },
   "outputs": [],
   "source": [
    "for col in df1.columns:\n",
    "    if col in COLS:\n",
    "        df1 = df1.withColumn(col,df1[col].cast('float'))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K4DkIkrdsRLW"
   },
   "outputs": [],
   "source": [
    "# Convert to float format\n",
    "def string_to_float(x):\n",
    "    return float(x)\n",
    "\n",
    "\n",
    "def condition(r):\n",
    "    if (0<= r <= 4):\n",
    "        label = \"low\"\n",
    "    elif(4< r <= 6):\n",
    "        label = \"medium\"\n",
    "    else:\n",
    "        label = \"high\"\n",
    "    return label\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "\n",
    "string_to_float_udf = udf(string_to_float, DoubleType())\n",
    "quality_udf = udf(lambda x: condition(x), StringType())\n",
    "\n",
    "df1 = df1.withColumn(\"quality\", quality_udf(\"quality\"))\n",
    "\n",
    "df1.show(5,True)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yATkPQ7Sqb4i"
   },
   "source": [
    "Transform the dataset to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gEYrQsApYWS"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors # !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "def transData(data):\n",
    "    return data.rdd.map(lambda r: [Vectors.dense(r[:-1]),r[-1]]).toDF(['features','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "otzdUC2qpkW2"
   },
   "outputs": [],
   "source": [
    "transformed = transData(df1)\n",
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dBj2fv50pqvU"
   },
   "source": [
    "**Deal with Categorical Label and Variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sRk--6Q4ykoW"
   },
   "source": [
    "Index labels, adding metadata to the label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wdoZsP0ypvTw"
   },
   "outputs": [],
   "source": [
    "labelIndexer = StringIndexer(inputCol='label',\n",
    "                             outputCol='indexedLabel').fit(transformed)\n",
    "labelIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ARIMB24WyrN4"
   },
   "source": [
    "Automatically identify categorical features, and index them.\n",
    "Set maxCategories so features with > 4 distinct values are treated as continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxhHo6Ftp1dh"
   },
   "outputs": [],
   "source": [
    "featureIndexer =VectorIndexer(inputCol=\"features\", \\\n",
    "                              outputCol=\"indexedFeatures\", \\\n",
    "                              maxCategories=4).fit(transformed)\n",
    "featureIndexer.transform(transformed).show(5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-wYZ1ZaVp6Sk"
   },
   "source": [
    "Split the data to training and test data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GEwQQOzzp5dS"
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = transformed.randomSplit([0.7, 0.3])\n",
    "\n",
    "trainingData.show(5,False)\n",
    "testData.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_MJ2Zo0jqTW-"
   },
   "source": [
    "Fit Multinomial logisticRegression Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDOtQT6xqS7A"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "logr = LogisticRegression(featuresCol='indexedFeatures', labelCol='indexedLabel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiitQo8pqch8"
   },
   "source": [
    "**Pipeline Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3oOaBoCDy4s9"
   },
   "source": [
    "Convert indexed labels back to original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYE75YvgqcCv"
   },
   "outputs": [],
   "source": [
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HOuvutmWy8Lg"
   },
   "source": [
    "Chain indexers and tree in a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tyXa1efAqh15"
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, logr,labelConverter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyVFFv5QzBa_"
   },
   "source": [
    "Train model.  This also runs the indexers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NjZr6SmLqlMI"
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o9WXSWqZqp22"
   },
   "source": [
    "Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSdCIGDcqqbV"
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"features\",\"label\",\"predictedLabel\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TLuaI2ziqupM"
   },
   "source": [
    "**Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NxdKr5QGqwEh"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vTdEqkQyq-Nr"
   },
   "source": [
    "**Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5l0Oz5Ykq-ur"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YwAlIWrdrDYX"
   },
   "outputs": [],
   "source": [
    "class_temp = predictions.select(\"label\").groupBy(\"label\")\\\n",
    "                        .count().sort('count', ascending=False).toPandas()\n",
    "class_temp = class_temp[\"label\"].values.tolist()\n",
    "class_names = map(str, class_temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0dvdl0mDzVwC"
   },
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ljv30359rFXs"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = predictions.select(\"label\")\n",
    "y_true = y_true.toPandas()\n",
    "\n",
    "y_pred = predictions.select(\"predictedLabel\")\n",
    "y_pred = y_pred.toPandas()\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_true, y_pred,labels=list(class_names))\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yec9CYnVzdhg"
   },
   "source": [
    "Plot non-normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4A0iWJUurHT6"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(class_names),\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aeO8-vJWzhK0"
   },
   "source": [
    "Plot normalized confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "84KRLPVtrKpk"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=list(class_names), normalize=True,\n",
    "                      title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPt+58dJrJP1y5YYL3cDDet",
   "include_colab_link": true,
   "name": "BBM469_Tutorial_8_Machine_Learning with Spark.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
